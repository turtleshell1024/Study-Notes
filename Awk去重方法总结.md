在 `awk` 中，可以使用数组来去重。常见的方法是借助 `awk` 的关联数组（类似哈希表）来存储唯一的键，从而过滤掉重复的行或字段。

---

## **1. 去重整行**

如果要去除重复的整行，可以这样做：

```bash
awk '!seen[$0]++' file.txt
```

**解释：**

* `$0` 代表整行内容。
* `seen[$0]` 是一个数组，键是行内容，值是出现次数。
* `!seen[$0]++` 逻辑：

  * 第一次遇到某行时，`seen[$0]` 默认为 0，`!0` 为 `true`，因此打印该行，并将 `seen[$0]` 递增 1。
  * 之后再遇到相同的行，`seen[$0]` 已递增，不为 0，`!seen[$0]` 为 `false`，不会打印。

---

## **2. 按列去重（基于某一列的唯一值）**

如果想基于**某一列**去重（例如第一列），可以使用：

```bash
awk '!seen[$1]++' file.txt
```

如果需要基于**某几列**去重，可以拼接字段：

```bash
awk '!seen[$1,$2]++' file.txt
```

**解释：**

* `$1` 是第一列，`$2` 是第二列。
* `seen[$1,$2]` 作为键名，保证相同的 **第一列 + 第二列** 组合只出现一次。

---

## **3. 仅保留重复项**

如果想只打印**重复的行**（不包含首次出现的），可以这样：

```bash
awk 'seen[$0]++' file.txt
```

**解释：**

* `seen[$0]++` 逻辑：

  * `seen[$0]` 先被读取，然后递增。
  * **第一次** `seen[$0]` 为 0，`0` 被转换为 `false`，所以不会打印。
  * **第二次及以后** `seen[$0]` 为 1 或更高，非 0，即 `true`，会打印该行。

---

## **4. 仅保留唯一项（删除所有重复项）**

如果想**只保留出现一次的行**（完全去掉重复项），可以用：

```bash
awk '{count[$0]++} END {for (i in count) if (count[i] == 1) print i}' file.txt
```

**解释：**

* `count[$0]++` 统计行出现的次数。
* `END` 块中，遍历 `count` 数组，**只打印出现 1 次的行**。

---

## **5. 维持原顺序去重**

有时希望去重但保持原始顺序，可用：

```bash
awk '!seen[$0]++' file.txt
```

这个方法已经保证了顺序。

如果 `awk` 版本支持 `PROCINFO["sorted_in"]`，可以这样：

```bash
awk '!seen[$0]++ {order[++n] = $0} END {for (i = 1; i <= n; i++) print order[i]}' file.txt
```

这样 `order` 数组保存原始顺序，在 `END` 里按顺序打印。

---

## **示例数据**

假设 `file.txt` 内容如下：

```
apple 10
banana 20
apple 10
orange 30
banana 20
```

### **去重整行**

```bash
awk '!seen[$0]++' file.txt
```

**输出：**

```
apple 10
banana 20
orange 30
```

### **基于第一列去重**

```bash
awk '!seen[$1]++' file.txt
```

**输出：**

```
apple 10
banana 20
orange 30
```

（每种水果只保留第一次出现的行）

---

### **结论**

* `awk '!seen[$0]++'` 是**最简单**的整行去重方式。
* `awk '!seen[$1]++'` 用于**按列去重**。
* `awk 'seen[$0]++'` 用于**提取重复项**。
* `awk '{count[$0]++} END {for (i in count) if (count[i] == 1) print i}'` 用于**仅保留唯一行**。

你具体想处理哪种情况？
